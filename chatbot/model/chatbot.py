import json
import os
import logging
from dotenv import load_dotenv
from google import genai
from google.genai import types
from google.genai.errors import APIError
import chromadb

parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
dotenv_path = os.path.join(parent_dir, 'backend', '.env')
load_dotenv(dotenv_path=dotenv_path)

logging.basicConfig(level=logging.INFO)

class ChatBot:
    def __init__(self, data_file="UNUSED"):
        try:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY environment variable not set.")

            self.client = genai.Client(api_key=api_key)
            self.model = "gemini-1.5-flash"
            self.embedding_model = 'text-embedding-004'
            self.safety_settings = [
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
            ]

        except Exception as e:
            logging.error(f"Failed to initialize Gemini Client: {e}")
            raise

        try:
            CHROMA_PATH = "chroma_db"
            self.chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
            self.collection = self.chroma_client.get_collection(name="chatbot_knowledge")
            logging.info(f"Loaded Knowledge Vector Store with {self.collection.count()} documents.")
            if self.collection.count() == 0:
                logging.warning("Knowledge Vector Store r·ªóng. Vui l√≤ng ch·∫°y python index_data.py ƒë·ªÉ t·∫°o l·∫°i d·ªØ li·ªáu.")

        except Exception as e:
            logging.error(f"Failed to load ChromaDB knowledge collection: {e}. ƒê·∫£m b·∫£o ƒë√£ ch·∫°y index_data.py.")
            self.collection = None

        try:
            FILES_CHROMA_PATH = "chroma_db_files"
            self.files_chroma_client = chromadb.PersistentClient(path=FILES_CHROMA_PATH)
            try:
                self.files_collection = self.files_chroma_client.get_collection(name="uploaded_files")
                logging.info(f"Loaded Files Vector Store with documents.")
            except:
                self.files_collection = self.files_chroma_client.create_collection(
                    name="uploaded_files",
                    metadata={"hnsw:space": "cosine"}
                )
                logging.info("Created new Files Vector Store.")
        except Exception as e:
            logging.error(f"Failed to initialize files collection: {e}")
            self.files_collection = None

    def _build_system_instruction(self, context_type="knowledge") -> str:
        if context_type == "files":
            return (
                "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung c√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n. "
                "Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi **CH·ªà** d·ª±a tr√™n th√¥ng tin trong ph·∫ßn **NG·ªÆ C·∫¢NH (CONTEXT)** ƒë∆∞·ª£c cung c·∫•p t·ª´ c√°c file ƒë√£ upload. "
                "Lu√¥n tr√≠ch d·∫´n ngu·ªìn file khi tr·∫£ l·ªùi. "
                "N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y n√≥i r√µ l√† th√¥ng tin n√†y kh√¥ng c√≥ trong c√°c t√†i li·ªáu ƒë√£ t·∫£i l√™n.\n"
            )
        else:
            return (
                "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán, chuy√™n t∆∞ v·∫•n v·ªÅ **H·ªá th·ªëng Qu·∫£n l√Ω Minh ch·ª©ng (Evidence Management System)** "
                "t·∫°i VNUA. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng **CH·ªà** d·ª±a tr√™n ki·∫øn th·ª©c ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn **NG·ªÆ C·∫¢NH (CONTEXT)**. "
                "Kh√¥ng suy lu·∫≠n hay th√™m th√¥ng tin ngo√†i ng·ªØ c·∫£nh. "
                "N·∫øu c√¢u h·ªèi n·∫±m ngo√†i ph·∫°m vi, h√£y tr·∫£ l·ªùi ch√≠nh x√°c v√† duy nh·∫•t b·∫±ng c√¢u: 'Xin l·ªói, t√¥i ch·ªâ c√≥ th·ªÉ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn h·ªá th·ªëng qu·∫£n l√Ω minh ch·ª©ng.'\n"
            )

    def get_reply(self, message: str) -> str:
        if not self.collection:
            return "D·ªãch v·ª• AI ho·∫∑c Kho Vector ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra API Key v√† ƒë·∫£m b·∫£o ƒë√£ ch·∫°y index_data.py."

        try:
            embedding_response = self.client.models.embed_content(
                model=self.embedding_model,
                contents=[message]
            )
            query_embedding = embedding_response.embedding

            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=3,
                include=['documents', 'distances', 'metadatas']
            )

            retrieved_documents = results['documents'][0]
            distances = results['distances'][0]

        except Exception as e:
            logging.error(f"Error during Knowledge Vector Retrieval: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t√¨m ki·∫øm trong kho ki·∫øn th·ª©c. Vui l√≤ng th·ª≠ l·∫°i."

        context_chunks = []
        for doc, dist in zip(retrieved_documents, distances):
            similarity_score = 1 - dist
            context_chunks.append(f"[Score: {similarity_score:.3f}] - {doc}")

        context = "\n".join(context_chunks)

        system_prompt = self._build_system_instruction("knowledge")

        final_prompt = (
            f"**NG·ªÆ C·∫¢NH (CONTEXT) - Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin n√†y:**\n"
            f"{context}\n\n"
            f"**C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG (USER QUESTION):** {message}\n"
            "**TR·∫¢ L·ªúI:**"
        )

        try:
            model = self.client.models.generate_content(
                model=self.model,
                contents=final_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    temperature=0.3,
                    max_output_tokens=250,
                    safety_settings=self.safety_settings
                )
            )

            reply = model.text.strip()

            unrelated_phrase = "xin l·ªói, t√¥i ch·ªâ c√≥ th·ªÉ h·ªó tr·ª£ c√°c v·∫•n ƒë·ªÅ li√™n quan ƒë·∫øn h·ªá th·ªëng qu·∫£n l√Ω minh ch·ª©ng"
            if unrelated_phrase in reply.lower():
                return "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi n√†y v√¨ n√≥ kh√¥ng li√™n quan ƒë·∫øn H·ªá th·ªëng Qu·∫£n l√Ω Minh ch·ª©ng. Vui l√≤ng ƒë∆∞a ra c√¢u h·ªèi ƒë√∫ng ho·∫∑c ch·ªçn t·ª´ c√°c g·ª£i √Ω."

            return reply

        except APIError as e:
            logging.error(f"Error calling Gemini API: {e}")
            raise RuntimeError("Gemini API call failed.")
        except Exception as e:
            logging.error(f"Error in get_reply: {e}")
            raise RuntimeError("Gemini API call failed due to an unknown error.")

    def get_reply_from_files(self, message: str) -> str:
        if not self.files_collection:
            return "D·ªãch v·ª• AI ho·∫∑c Kho Files ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh."

        try:
            embedding_response = self.client.models.embed_content(
                model=self.embedding_model,
                contents=[message]
            )
            query_embedding = embedding_response.embedding

            results = self.files_collection.query(
                query_embeddings=[query_embedding],
                n_results=5,
                include=['documents', 'distances', 'metadatas']
            )

            if not results['documents'][0]:
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c√°c file ƒë√£ upload. Vui l√≤ng upload file ch·ª©a th√¥ng tin b·∫°n c·∫ßn h·ªèi."

            retrieved_documents = results['documents'][0]
            distances = results['distances'][0]
            metadatas = results['metadatas'][0]

        except Exception as e:
            logging.error(f"Error during Files Vector Retrieval: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi t√¨m ki·∫øm trong c√°c file ƒë√£ upload."

        context_chunks = []
        files_referenced = set()

        for doc, dist, metadata in zip(retrieved_documents, distances, metadatas):
            similarity_score = 1 - dist
            filename = metadata.get('filename', 'Unknown')
            files_referenced.add(filename)
            context_chunks.append(
                f"[File: {filename} | Score: {similarity_score:.3f}]\n{doc}"
            )

        context = "\n\n".join(context_chunks)
        files_list = ", ".join(files_referenced)

        system_prompt = self._build_system_instruction("files")

        final_prompt = (
            f"**NG·ªÆ C·∫¢NH T·ª™ C√ÅC FILE ƒê√É UPLOAD:**\n"
            f"C√°c file ƒë∆∞·ª£c tham kh·∫£o: {files_list}\n\n"
            f"{context}\n\n"
            f"**C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG:** {message}\n"
            f"**TR·∫¢ L·ªúI (nh·ªõ tr√≠ch d·∫´n ngu·ªìn file khi c·∫ßn thi·∫øt):**"
        )

        try:
            model = self.client.models.generate_content(
                model=self.model,
                contents=final_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    temperature=0.3,
                    max_output_tokens=400,
                    safety_settings=self.safety_settings
                )
            )

            reply = model.text.strip()

            if files_referenced:
                reply += f"\n\nüìé *Ngu·ªìn tham kh·∫£o: {files_list}*"

            return reply

        except APIError as e:
            logging.error(f"Error calling Gemini API for files: {e}")
            return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."
        except Exception as e:
            logging.error(f"Error in get_reply_from_files: {e}")
            return "Xin l·ªói, ƒë√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën."

    def summarize_text(self, text: str, max_length: int = 500) -> str:
        try:
            if len(text) > 10000:
                text = text[:10000] + "..."

            prompt = f"""H√£y t√≥m t·∫Øt n·ªôi dung sau ƒë√¢y m·ªôt c√°ch ng·∫Øn g·ªçn v√† s√∫c t√≠ch trong kho·∫£ng {max_length} k√Ω t·ª±.
            T·∫≠p trung v√†o c√°c √Ω ch√≠nh v√† th√¥ng tin quan tr·ªçng nh·∫•t.
            
            N·ªôi dung c·∫ßn t√≥m t·∫Øt:
            {text}
            
            T√≥m t·∫Øt:"""

            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=max_length // 4,
                    safety_settings=self.safety_settings
                )
            )

            return response.text.strip()

        except Exception as e:
            logging.error(f"Error in summarize_text: {e}")
            return "L·ªói khi t√≥m t·∫Øt vƒÉn b·∫£n"

    def get_contextual_followup(self, last_reply: str) -> list[str]:
        if "xin l·ªói" in last_reply.lower() or "kh√¥ng t√¨m th·∫•y" in last_reply.lower():
            return []

        prompt = (
            f"D·ª±a tr√™n c√¢u tr·∫£ l·ªùi cu·ªëi c√πng n√†y: '{last_reply}'. "
            "H√£y ƒë·ªÅ xu·∫•t 3 c√¢u h·ªèi ti·∫øp theo ng·∫Øn g·ªçn (d∆∞·ªõi 10 t·ª´) m√† ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi. "
            "Ch·ªâ tr·∫£ l·ªùi b·∫±ng 3 c√¢u h·ªèi, m·ªói c√¢u n·∫±m tr√™n m·ªôt d√≤ng, kh√¥ng c√≥ s·ªë th·ª© t·ª± hay k√Ω t·ª± ƒë·∫∑c bi·ªát."
        )

        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.5,
                    max_output_tokens=100,
                )
            )

            suggestions_text = response.text.strip()
            suggestions = [s.strip() for s in suggestions_text.split('\n') if s.strip()]

            return suggestions[:3]

        except APIError as e:
            logging.error(f"Error generating follow-up suggestions: {e}")
            return []
        except Exception as e:
            logging.error(f"Error in get_contextual_followup: {e}")
            return []